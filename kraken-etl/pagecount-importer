#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" Imports any missing hourly pagecounts from dumps.wikimedia.org into
    a time bucketed HDFS directory hierarchy.
    Considers hours since either:
        1. The first hour imported
        2. The hour specified as --start
        3. The first hour of today's date

Usage: pagecount-importer [options] <datadir>

<datadir> is a directory in hdfs under which to import pagecounts/hourly

Options:
    -h --help                           Show this help message and exit
    -v --verbose                        Print out DEBUG messages
    -n --dry-run                        Just show what would be imported
    -s --start=<start_hour>             The first hour to use if nothing is imported yet
                                        Format: %Y.%m.%d_%H
"""
import os
import re
import urllib2
import traceback
import logging
from docopt import docopt
from datetime import datetime, timedelta, date
from util import (
    HdfsDatasetUtils, diff_datewise, timestamps_to_now, sh,
)

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)

    datadir                 = arguments['<datadir>']
    verbose                 = arguments['--verbose']
    dry_run                 = arguments['--dry-run']

    log_level = logging.INFO
    if verbose:
        log_level = logging.DEBUG

    logging.basicConfig(level=log_level,
                        format='%(asctime)s %(levelname)-6s %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')

    start = arguments['--start']
    if start:
        start = datetime.strptime(start, '%Y.%m.%d_%H')
    else:
        start = date.today()

target = HdfsDatasetUtils(datadir)
dataset = 'pagecounts'
hdfs_target = target.dataset_dir(dataset, interval='hourly')

relative_dir_format ='%Y/%m/%d/%H'
hdfs_dir_format = os.path.join(hdfs_target, relative_dir_format)
hdfs_file_format = os.path.join(hdfs_dir_format, 'pagecounts-%Y.%m.%d_%H')

imported = target.partitions(dataset)
if not imported or not imported[0]:
    imported = [start.strftime(relative_dir_format)]
first_hour = datetime.strptime(min(imported), relative_dir_format)
hours_to_attempt = timestamps_to_now(first_hour, timedelta(hours=1))

hours_missing = diff_datewise(
    hours_to_attempt,
    imported,
    right_parse=relative_dir_format,
)[0]

month_cache = dict()
hours_to_import = dict()
ymdh_format = '%Y%m%d%H'
link_format = 'pagecounts-%Y%m%d-%H%M%S.gz'
base_url = 'http://dumps.wikimedia.org/other/pagecounts-raw/%Y/%Y-%m/'

for timestamp in hours_missing:
    url_for_month = timestamp.strftime(base_url)
    ymdh = timestamp.strftime(ymdh_format)

    if not url_for_month in month_cache:
        month_cache[url_for_month] = dict()
        res = urllib2.urlopen(url_for_month)
        if res.code == 200:
            html = res.read()
            hour_links = re.findall('(?<=href=")pagecounts-[^"]*.gz', html)
            for hour_link in hour_links:
                parsed = datetime.strptime(hour_link, link_format)
                parsed_ymdh = parsed.strftime(ymdh_format)
                month_cache[url_for_month][parsed_ymdh] = \
                    url_for_month + hour_link

    if ymdh in month_cache[url_for_month]:
        hours_to_import[timestamp] = month_cache[url_for_month][ymdh]

for hour_to_import, link in hours_to_import.iteritems():
    logging.info('Importing {0} From {1} To {2}'.format(
        hour_to_import,
        link,
        hour_to_import.strftime(hdfs_file_format)
    ))
    if dry_run:
        logging.info('Skipping due to DRY RUN option')
        continue

    hdfs_dir = hour_to_import.strftime(hdfs_dir_format)
    try:
        sh('hdfs dfs -mkdir -p {0}'.format(hdfs_dir))
        result = sh('wget -O - {0} | gunzip -c | hdfs dfs -put - {1}'.format(
            link, hour_to_import.strftime(hdfs_file_format)
        ))
    except Exception as e:
        logging.error(traceback.format_exc())
        sh('hdfs dfs -rm {0}'.format(hdfs_dir))
