# Configures a coordinator to manage automatically dropping old Hive partitions
# for a webrequest table, and and deleting their respective data locations in
# HDFS. Any of the following properties are overidable with -D.
# The 'webrequest_source' property is required and must be set on the CLI when
# submitting this coordinator.
#
# Usage:
# oozie job --Dwebrequest_source=mobile -submit -config oozie/webrequest/drop/coordinator.properties.
#
# NOTE:  The $oozieDirectory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.


# hadoopMaster will be used for <name-node> and <job-tracker> elements.
hadoopMaster                      = analytics1010.eqiad.wmnet
nameNode                          = hdfs://${hadoopMaster}:8020
jobTracker                        = ${hadoopMaster}:8032
queueName                         = standard

# Base path in HDFS to oozie files.
# Other files will be used relative to this path.
oozieDirectory                    = ${nameNode}/wmf/kraken/current/oozie

# HDFS path to workflow to run.
workflowFile                      = ${oozieDirectory}/util/hive/partition/add/workflow.xml

# HDFS path to webrequest dataset definition
datasetsFile                      = ${oozieDirectory}/webrequest/datasets.xml

# Initial import time of the webrequest dataset.
# Note:  This is intentionally set to 1440 hours
#        after the add partition coordinator's
#        initial instance.  Each drop workflow app
#        will remove the partition 1440 hours before
#        the current instance.  So if we want to delete
#        partitions that old, we need to wait until
#        that much time has passed.
startTime                         = 2014-05-31T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stopTime                          = 3000-01-01T00:00Z

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                     = ${oozieDirectory}/util/hive/hive-site.xml

# HDFS path to SerDe jar that the webrequest table uses.
serde_jar                         = ${nameNode}/wmf/kraken/artifacts/hcatalog-core-0.5.0-cdh4.3.1.jar

# Hive database name.
database                          = wmf

# Hive table name.
table                             = webrequest

# HDFS path to directory where webrequest data is time bucketed.
dataDirectory                     = ${nameNode}/wmf/data/external/${table}/webrequest_${webrequest_source}/hourly

# Coordintator to start
oozie.coord.application.path      = ${oozieDirectory}/webrequest/partition/add/coordinator.xml
oozie.libpath                     = ${nameNode}/user/oozie/share/lib
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true